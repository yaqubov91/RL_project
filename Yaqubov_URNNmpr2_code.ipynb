{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pylab as plb\n",
    "import numpy as np\n",
    "\n",
    "class MountainCar():\n",
    "    \"\"\"A mountain-car problem.\n",
    "\n",
    "    For the miniproject, you are not meant to change the default parameters\n",
    "    (mass of the car, etc.)\n",
    "\n",
    "    Usage: \n",
    "        >>> mc = MountainCar()\n",
    "        \n",
    "        Set the agent to apply a rightward force (positive in x)\n",
    "        >>> mc.apply_force(+1) # the actual value doesn't mattter, only the sign\n",
    "        \n",
    "        Run an \"agent time step\" of 1s with 0.01 s integration time step\n",
    "        >>> mc.simulate_timesteps(n = 100, dt = 0.01)\n",
    "        \n",
    "        Check the state variables of the agent, and the reward\n",
    "        >>> print mc.x, mc.x_d, mc.R\n",
    "\n",
    "        At some point, one might want to reset the position/speed of the car\n",
    "        >>> mc.reset()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, g = 10.0, d = 100.0, H = 10., m = 10.0, \n",
    "                force_amplitude = 3.0, reward_amplitude = 1., \n",
    "                 reward_threshold = 0.0):\n",
    "        \n",
    "        # set internal parameters from constructor call\n",
    "        self.g = g # gravitational constant\n",
    "        self.d = d # minima location\n",
    "        self.H = H # height of the saddle point\n",
    "        self.m = m # mass of the car\n",
    "        self.force_amplitude = force_amplitude # amplitude of the force applied by the engine\n",
    "        self.reward_amplitude = reward_amplitude # value of the reward\n",
    "        self.reward_threshold = reward_threshold # x-axis threshold for the obtention of reward\n",
    "\n",
    "        # reset the car variables\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the mountain car to a random initial position.\n",
    "        \"\"\"\n",
    "\n",
    "        # set position to range [-130; -50]\n",
    "        self.x = 80 * np.random.rand() - 130.0 \n",
    "        #self.x = -60.0\n",
    "        #print('Car init. at x=', self.x)\n",
    "        \n",
    "        # set x_dot to range [-5; 5]\n",
    "        self.x_d = 10.0 * np.random.rand() - 5.0\n",
    "        # reset reward\n",
    "        self.R = 0.0\n",
    "        # reset time\n",
    "        self.t = 0.0\n",
    "        # reset applied force\n",
    "        self.F = 0.0\n",
    "\n",
    "    def apply_force(self, direction):\n",
    "        \"\"\"Apply a force to the car.\n",
    "\n",
    "        Only three values of force are possible: \n",
    "            right (if direction > 0), \n",
    "            left (direction < 0) or\n",
    "            no force (direction = 0).\\\n",
    "        \"\"\"\n",
    "        self.F = np.sign(direction) * self.force_amplitude\n",
    "\n",
    "    def _h(self, x):\n",
    "        \"\"\"Return the value of the landscape function h in x.\n",
    "        \"\"\"\n",
    "        return (x - self.d)**2 * (x + self.d)**2 / ((self.d**4/self.H)+x**2)\n",
    "        \n",
    "    def _h_prime(self, x):\n",
    "        \"\"\"Return the value of the first derivative of the landscape function h in x.\n",
    "        \"\"\"\n",
    "        c = self.d**4/self.H\n",
    "        return 2 * x * (x**2 - self.d**2) * (2*c + self.d**2  + x**2) / (c+x**2)**2\n",
    "\n",
    "    def _h_second(self, x):\n",
    "        \"\"\"Return the value of the second derivative of the landscape function h in x.\n",
    "        \"\"\"\n",
    "        c = self.d**4/self.H\n",
    "        return 2 * (\n",
    "            - 2 * c**2 * (self.d**2 - 3*x**2) \n",
    "            + c * (-self.d**4 + 6*self.d**2 * x**2 + 3*x**4)\n",
    "            + 3 * self.d**4 * x**2\n",
    "            + x**6\n",
    "        ) / (c + x**2)**3\n",
    "\n",
    "    def _energy(self, x, x_d):\n",
    "        \"\"\"Return the total energy of the car with variable x and x_d.\n",
    "        \"\"\"\n",
    "        # note that v and x dot are not the same: v includes the y direction!\n",
    "        return self.m * (self.g * self._h(x) + 0.5 * (1 + self._h_prime(x)**2) * x_d**2) \n",
    "\n",
    "    def simulate_timesteps(self, n = 1, dt = 0.1):\n",
    "        \"\"\"Simulate the car dynamics for n timesteps of length dt.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(n):\n",
    "            self._simulate_single_timestep(dt)\n",
    "        \n",
    "        self.t += n*dt\n",
    "\n",
    "        # check for rewards\n",
    "        self.R = self._get_reward()\n",
    "\n",
    "    def _simulate_single_timestep(self, dt):\n",
    "        \"\"\"Simulate the car dynamics for a single timestep.\n",
    "        \"\"\"\n",
    "\n",
    "        # calculate the second derivative of x (horiz. acceleration)\n",
    "        alpha = np.arctan(self._h_prime(self.x))\n",
    "        x_dd = np.cos(alpha) * (self.F / self.m - np.sin(alpha) * (self.g + self._h_second(self.x) * self.x_d**2))\n",
    "\n",
    "        # update the position and velocity on the x axis\n",
    "        self.x += self.x_d * dt + 0.5 * x_dd * dt**2\n",
    "        self.x_d += x_dd * dt\n",
    "\n",
    "    def _get_reward(self):\n",
    "        \"\"\"Check for and return reward.\n",
    "        \"\"\"\n",
    "\n",
    "        # if there's already a reward, we stick to it\n",
    "        if self.R > 0.0:\n",
    "            return self.R\n",
    "        # have we crossed the threshold?\n",
    "        if self.x >= self.reward_threshold:\n",
    "            return self.reward_amplitude\n",
    "\n",
    "        # else no reward\n",
    "        return 0.0\n",
    "\n",
    "class MountainCarViewer():\n",
    "    \"\"\"Display the state of a MountainCar instance.\n",
    "    \n",
    "    Usage: \n",
    "        >>> mc = MountainCar()\n",
    "\n",
    "        >>> mv = MoutainCarViewer(mc)\n",
    "\n",
    "        Turn matplotlib's \"interactive mode\" on and create figure\n",
    "        >>> plb.ion()\n",
    "        >>> mv.create_figure(n_steps = 200, max_time = 200)\n",
    "        \n",
    "        This forces matplotlib to draw the fig. before the end of execution\n",
    "        >>> plb.draw()\n",
    "        \n",
    "        Simulate the MountainCar, visualizing the state\n",
    "        >>> for n in range(200):\n",
    "        >>>     mc.simulate_timesteps(100,0.01)\n",
    "        >>>     mv.update_figure()\n",
    "        >>>     plb.draw()\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mountain_car):\n",
    "        assert isinstance(mountain_car, MountainCar), \\\n",
    "                'Argument to MoutainCarViewer() must be a MountainCar instance'\n",
    "        self.mountain_car = mountain_car\n",
    "\n",
    "    def create_figure(self, n_steps, max_time, f = None):\n",
    "        \"\"\"Create a figure showing the progression of the car.\n",
    "        \n",
    "        Call update_car_state susequently to update this figure.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_steps  -- number of times update_car_state will be called.\n",
    "        max_time -- the time the trial will last (to scale the plots).\n",
    "        f        -- (optional) figure in which to create the plots.\n",
    "        \"\"\"\n",
    "\n",
    "        if f is None:\n",
    "            self.f = plb.figure()\n",
    "        else:\n",
    "            self.f = f\n",
    "\n",
    "        # create the to store the arrays\n",
    "        self.times = np.zeros(n_steps + 1)\n",
    "        self.positions = np.zeros((n_steps + 1,2))\n",
    "        self.forces = np.zeros(n_steps + 1)\n",
    "        self.energies = np.zeros(n_steps + 1)\n",
    "\n",
    "        # Fill the initial values\n",
    "        self.i = 0\n",
    "        self._get_values()\n",
    "\n",
    "        # create the energy landscape plot\n",
    "        self.ax_position = plb.subplot(2,1,1)\n",
    "        self._plot_energy_landscape(self.ax_position)\n",
    "        self.h_position = self._plot_positions()\n",
    "\n",
    "        # create the force plot\n",
    "        self.ax_forces = plb.subplot(2,2,3)\n",
    "        self.h_forces = self._plot_forces()\n",
    "        plb.axis(xmin = 0, xmax = max_time, \n",
    "                 ymin = -1.1 * self.mountain_car.force_amplitude,\n",
    "                 ymax = 1.1 * self.mountain_car.force_amplitude)\n",
    "        \n",
    "        # create the energy plot\n",
    "        self.ax_energies = plb.subplot(2,2,4)\n",
    "        self.h_energies = self._plot_energy()\n",
    "        plb.axis(xmin = 0, xmax = max_time, \n",
    "                 ymin = 0.0, ymax =1000.)\n",
    "\n",
    "    def update_figure(self):\n",
    "        \"\"\"Update the figure.\n",
    "\n",
    "        Assumes the figure has already been created with create_figure.\n",
    "        \"\"\"\n",
    "\n",
    "        # increment \n",
    "        self.i += 1\n",
    "        assert self.i < len(self.forces), \\\n",
    "                \"update_figure was called too many times.\"\n",
    "\n",
    "        # get the new values from the car\n",
    "        self._get_values()\n",
    "\n",
    "        # update the plots\n",
    "        self._plot_positions(self.h_position)\n",
    "        self._plot_forces(self.h_forces)\n",
    "        self._plot_energy(self.h_energies)\n",
    "\n",
    "    def _get_values(self):\n",
    "        \"\"\"Retrieve the relevant car variables for the figure.\n",
    "        \"\"\"\n",
    "        self.times[self.i] = self.mountain_car.t\n",
    "        self.positions[self.i,0] = self.mountain_car.x\n",
    "        self.positions[self.i,1] = self.mountain_car.x_d\n",
    "        self.forces[self.i] = self.mountain_car.F\n",
    "        self.energies[self.i] = self.mountain_car._energy(\n",
    "            self.mountain_car.x, self.mountain_car.x_d)\n",
    "\n",
    "    def _plot_energy_landscape(self, ax = None):\n",
    "        \"\"\"plot the energy landscape for the mountain car in 2D.\n",
    "\n",
    "        Returns the axes instance created. Use plot_energy_landscape to let \n",
    "        the module decide whether you have the right modules for 3D plotting.\n",
    "        \"\"\"\n",
    "\n",
    "        # create coordinates for a grid in the x-x_dot space\n",
    "        X = np.linspace(-160, 160, 61)\n",
    "        XD = np.linspace(-20, 20, 51)\n",
    "        X,XD = np.meshgrid(X , XD)\n",
    "\n",
    "        # calculate the energy in each point of the grid\n",
    "        E = self.mountain_car._energy(X, XD)\n",
    "\n",
    "        # display the energy as an image\n",
    "        if ax is None:\n",
    "            f = plb.figure()\n",
    "            ax = plb.axes()\n",
    "        \n",
    "        C = ax.contourf(X,XD, E,100)\n",
    "        ax.set_xlabel('$x$')\n",
    "        ax.set_ylabel('$\\dot x$')\n",
    "        cbar = plb.colorbar(C)\n",
    "        cbar.set_label('$E$')\n",
    "\n",
    "        return ax\n",
    "\n",
    "    def _plot_positions(self, handles = None):\n",
    "        \"\"\"plot the position and trajectory of the car in state space.\n",
    "        \"\"\"\n",
    "\n",
    "        # choose the color of the point according to the force direction\n",
    "        color = ['r', 'w', 'g'][1 + int(np.sign(self.mountain_car.F))]\n",
    "\n",
    "        if handles is None:\n",
    "            # create the plots\n",
    "            handles = [] # keep the plot objects in this list\n",
    "            handles.append(plb.plot(\n",
    "                np.atleast_1d(self.positions[:self.i+1,0]),\n",
    "                np.atleast_1d(self.positions[:self.i+1,1]),\n",
    "                ',k'\n",
    "            )[0])\n",
    "            handles.append(plb.plot(\n",
    "                np.atleast_1d(self.positions[self.i,0]),\n",
    "                np.atleast_1d(self.positions[self.i,1]),\n",
    "                'o' + color,\n",
    "                markeredgecolor = 'none',\n",
    "                markersize = 9,                \n",
    "            )[0])\n",
    "            return tuple(handles)\n",
    "        else:\n",
    "            # update the plots\n",
    "            handles[0].set_xdata(np.atleast_1d(self.positions[:self.i+1,0]))\n",
    "            handles[0].set_ydata(np.atleast_1d(self.positions[:self.i+1,1]))\n",
    "            handles[1].set_xdata(np.atleast_1d(self.positions[self.i,0]))\n",
    "            handles[1].set_ydata(np.atleast_1d(self.positions[self.i,1]))\n",
    "            handles[1].set_color(color)\n",
    "            return handles\n",
    "\n",
    "    def _plot_forces(self, handle = None):\n",
    "        \"\"\"plot the force applied by the car vs time.\n",
    "        \"\"\"\n",
    "        # create the plots\n",
    "        if handle is None:\n",
    "            handle = plb.plot(\n",
    "                np.atleast_1d(self.times[:self.i+1]),\n",
    "                np.atleast_1d(self.forces[:self.i+1]),\n",
    "                ',k',\n",
    "            )[0]\n",
    "\n",
    "            plb.xlabel('$t$')\n",
    "            plb.ylabel('$F$')\n",
    "            return handle\n",
    "        else:\n",
    "            # update the plot\n",
    "            handle.set_xdata(np.atleast_1d(self.times[:self.i+1]))\n",
    "            handle.set_ydata(np.atleast_1d(self.forces[:self.i+1]))\n",
    "            return handle\n",
    "\n",
    "    def _plot_energy(self, handle = None):\n",
    "        \"\"\"plot the energy of the car vs time.\n",
    "        \"\"\"\n",
    "        # create the plots\n",
    "        if handle is None:\n",
    "            handle = plb.plot(\n",
    "                np.atleast_1d(self.times[:self.i+1]),\n",
    "                np.atleast_1d(self.energies[:self.i+1]),\n",
    "                'k',\n",
    "                linewidth = 0.5\n",
    "            )[0]\n",
    "\n",
    "            plb.xlabel('$t$')\n",
    "            plb.ylabel('$E$')\n",
    "            return handle\n",
    "        else:\n",
    "            # update the plot\n",
    "            handle.set_xdata(np.atleast_1d(self.times[:self.i+1]))\n",
    "            handle.set_ydata(np.atleast_1d(self.energies[:self.i+1]))\n",
    "            return handle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pylab as plb\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"A not so good agent for the mountain-car task.\"\"\"\n",
    "\n",
    "    def __init__(self, weights, neurons, sigma, eta=0.1, lamb=0.95, tau=1, gamma=0.95, actions=np.array([-1,0,1]), mountain_car = None):\n",
    "\n",
    "        if mountain_car is None:\n",
    "            self.mountain_car = MountainCar()\n",
    "        else:\n",
    "            self.mountain_car = mountain_car\n",
    "\n",
    "        self.weights = weights\n",
    "        self.actions = actions\n",
    "        self.neurons = neurons\n",
    "        self.sigma = sigma\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.eta = eta\n",
    "        self.e = np.zeros(self.weights.shape)\n",
    "        self.lamb = lamb\n",
    "\n",
    "    def activity(self, state):\n",
    "        r = np.exp(-((state[0]-self.neurons[:,:,0])**2/self.sigma[0]**2)-((state[1]-self.neurons[:,:,1])**2/self.sigma[1]**2))\n",
    "        return r\n",
    "\n",
    "    def computeQ(self, r):\n",
    "        Q = np.zeros((len(self.actions),))\n",
    "        for a in range(Q.shape[0]):\n",
    "            Q[a] = np.sum(self.weights[:,:,a]*r)\n",
    "        return Q\n",
    "\n",
    "    def computeProba(self, Q):\n",
    "        norm = np.sum(np.exp(Q/self.tau))\n",
    "        P = np.exp(Q/self.tau)/norm\n",
    "        return P\n",
    "\n",
    "    def softmax(self, probabilities):\n",
    "        return np.random.choice(self.actions, 1, p=probabilities)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        #get the input layer neurons\n",
    "        r = self.activity(state)\n",
    "        #compute the Q values:\n",
    "        Q = self.computeQ(r)\n",
    "        P = self.computeProba(Q)\n",
    "        a_star = self.softmax(P)\n",
    "\n",
    "        return a_star, Q[a_star], r\n",
    "\n",
    "    def computeDelta(self):\n",
    "        delta = self.reward + self.gamma*self.Q_next - self.Q\n",
    "        return delta\n",
    "\n",
    "\n",
    "    def learn(self):\n",
    "\n",
    "        delta = self.computeDelta()\n",
    "        # get action idx\n",
    "        idx = np.nonzero(self.actions==self.action)[0][0]\n",
    "        # update the trace\n",
    "        self.e = self.lamb*self.e \n",
    "        self.e[:,:,idx] += self.r\n",
    "\n",
    "        # update the weights\n",
    "        self.weights += self.eta*delta*self.e\n",
    "        self.weights[np.nonzero(self.weights<0)] = 0\n",
    "\n",
    "\n",
    "    ############################################\n",
    "    #### Functions for visualization of learning behavior\n",
    "\n",
    "    def get_info(self):\n",
    "        return self.weights, self.mountain_car.t\n",
    "\n",
    "    def plot_quiver(self):\n",
    "\n",
    "        a_star = np.zeros((self.weights.shape[0],self.weights.shape[1]))\n",
    "        for i in range(self.weights.shape[0]):\n",
    "            for j in range(self.weights.shape[1]):\n",
    "                a_star[i,j] = self.actions[np.argmax(self.weights[i,j,:])]\n",
    "\n",
    "        plb.imshow(a_star, aspect='auto', extent=[-150,30,-15,15], interpolation='nearest')\n",
    "        plb.colorbar()\n",
    "        plb.xlabel(\"position x [m]\")\n",
    "        plb.ylabel(\"velocity v [m/s]\")\n",
    "        plb.draw()\n",
    "        plb.show()\n",
    "\n",
    "    def plot_weights(self):\n",
    "        for i in range(3):\n",
    "            plb.subplot(1,3,i+1)\n",
    "            plb.imshow(self.weights[:,:,i], interpolation='nearest')\n",
    "            plb.colorbar(fraction=0.06, pad = 0.06)\n",
    "            plb.xlabel(\"position x [m]\")   \n",
    "            plb.tight_layout()\n",
    "            if i == 0:\n",
    "                plb.ylabel(\"velocity v [m/s]\")\n",
    "        plb.draw()\n",
    "        plb.show()\n",
    "    \n",
    "    def plot_eligibilityTrace(self):\n",
    "        for i in range(3):\n",
    "            plb.subplot(1,3,i+1)\n",
    "            plb.imshow(self.e[:,:,i],interpolation='nearest')\n",
    "            plb.colorbar(fraction=0.06, pad = 0.06)\n",
    "            plb.xlabel(\"position x [m]\")\n",
    "            plb.tight_layout()\n",
    "            if i == 0:\n",
    "                plb.ylabel(\"velocity v [m/s]\")\n",
    "        plb.draw()\n",
    "        plb.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##################################################################\n",
    "    # Main function : run one agent\n",
    "    ##################################################################\n",
    "\n",
    "    def visualize_trial(self, n_steps = 10000):\n",
    "        \"\"\"Do a trial without learning, with display.\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_steps -- number of steps to simulate for\n",
    "        \"\"\"\n",
    "\n",
    "        # prepare for the visualization\n",
    "        #plb.ion()\n",
    "        #mv = MountainCarViewer(self.mountain_car) #mountaincar.MountainCarViewer(self.mountain_car)\n",
    "        #mv.create_figure(n_steps, n_steps)\n",
    "        #plb.draw()\n",
    "\n",
    "        # make sure the mountain-car is reset\n",
    "        self.mountain_car.reset()\n",
    "\n",
    "        # state initialization \n",
    "        self.state = np.array([self.mountain_car.x,self.mountain_car.x_d])\n",
    "        # choose the first action, save the Q value and the activity\n",
    "        self.action, self.Q, self.r = self.choose_action(self.state)\n",
    "\n",
    "        for n in range(n_steps):\n",
    "            #if n%1000==0:\n",
    "            #    print('\\rt =', self.mountain_car.t)\n",
    "            #    sys.stdout.flush()\n",
    "\n",
    "            # Apply force according to the chosen action\n",
    "            self.mountain_car.apply_force(self.action)\n",
    "            # simulate the timestep\n",
    "            self.mountain_car.simulate_timesteps(100, 0.01)\n",
    "\n",
    "            # observe the new state and the reward\n",
    "            self.state_next = np.array([self.mountain_car.x,self.mountain_car.x_d])\n",
    "            self.reward = self.mountain_car.R\n",
    "\n",
    "            # choose the next action\n",
    "            self.action_next, self.Q_next, self.r_next = self.choose_action(self.state_next)\n",
    "\n",
    "            # update the weights with respect to SARSA for continuous state space\n",
    "            self.learn()\n",
    "\n",
    "            # define next quantities as the new ones\n",
    "            self.action = self.action_next\n",
    "            self.Q = self.Q_next\n",
    "            self.state = self.state_next\n",
    "            self.r = self.r_next\n",
    "\n",
    "            # update the visualization\n",
    "            #mv.update_figure()\n",
    "            #plb.draw()\n",
    "\n",
    "            # check for rewards\n",
    "            if self.mountain_car.R > 0.0:\n",
    "                print(\"reward obtained at t=\", self.mountain_car.t)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def center_generate(s_x, s_v, x_min, x_max, v_min, v_max):\n",
    "\n",
    "    sigma = np.array([(x_max-x_min)/(s_x-1), (v_max-v_min)/(s_v-1)])\n",
    "    neurons = np.zeros((s_v,s_x,2))\n",
    "    for i in range(s_v):\n",
    "        for j in range(s_x):\n",
    "            neurons[i,j,:] = np.array([x_min + j*sigma[0], v_max - i*sigma[1]])\n",
    "\n",
    "    return neurons, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Geometric values for building neurons\n",
    "s_x, s_v = 20, 20 # number of neurons along position axis x and velocity axis v\n",
    "n_neuron = s_x*s_v # total number of neurons\n",
    "x_min, x_max = -150, 30\n",
    "v_min, v_max = -15, 15\n",
    "# Build the net of input neurons\n",
    "neurons, sigma = center_generate(s_x, s_v, x_min, x_max, v_min, v_max)\n",
    "\n",
    "n_action = 3 # number of possible actions {-1: backward force, 0: no force, 1: forward force}\n",
    "n_step = 10000 # maximum number of steps\n",
    "n_agent = 10 # number of agent\n",
    "n_period = 21\n",
    "# vectors that allow to save informations across agents and simulations\n",
    "agent_times = np.zeros((n_period,n_agent)) # times needed to solve the task\n",
    "\n",
    "# initialize the weights that are learned across several agents\n",
    "#weights = np.zeros((s_v, s_x, n_action, n_agent)) +1\n",
    "weights = 0.001 * np.random.rand(s_v, s_x, n_action, n_agent) + 0.1\n",
    "# The parameters that user can change to analyse their effect on learning behavior\n",
    "eta=0.05\n",
    "lamb=0.95\n",
    "tau=0.1\n",
    "#tau = np.logspace(1,-2, n_period)\n",
    "\n",
    "# Main loop running agents based on the previous weights\n",
    "for p in range(n_period):\n",
    "    print(\"Period\", p+1, \"/\", n_period)\n",
    "    for a in range(n_agent):\n",
    "        agent = Agent(weights[:,:,:,a], neurons, sigma, eta, lamb, tau)\n",
    "        agent.visualize_trial(n_step)\n",
    "        if p%20 == 0:\n",
    "            agent.plot_quiver()\n",
    "            agent.plot_weights()\n",
    "            #agent.plot_eligibilityTrace()\n",
    "        weights[:,:,:,a], agent_times[p,a] = agent.get_info()\n",
    "\n",
    "#np.save('W_lamb1_tau1.npy',weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plb.plot(np.mean(agent_times,1))\n",
    "plb.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
